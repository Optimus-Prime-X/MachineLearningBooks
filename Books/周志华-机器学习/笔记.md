# 第二章 模型评估与选择
## 2.1 经验误差与过拟合

引起过拟合的可能因素：

- 学习能力过强
  - 决策树扩展分支
  - 神经网络增加训练轮数

## 2.2 模型评估

测试集与训练集互斥
  
有几种方法划分：

- 留出法——尽可能保证分布一致性
- 交叉验证法——十折交叉验证

``` python
sklearn.model_selection.Kflod(n_splits = 3, shuffle = False, random_state = None)
```

n_splits——k折的值
shuffle——对数据是否随机搅动
random_state——随机种子

训练集训练，测试集测试，验证准确度去了是一个<font color = #ff0000> 测试准确度</font>的测量,而测试准确度是一个<font color = #ff0000>高方差估计方法</font>。

- 自助法（bootstrapping）
  
  有放回抽样，抽样到和原始样本同样等级
  适用于<font color = #aaaa>数据集较小</font>的时候

调参与最终模型——计算与性能折中

## 2.3 性能度量

模型好坏：任务需求，算法，数据！
学习器f的预测结果f(x)与y比较

- 回归任务常用“均方误差”：

### 2.3.1 错误率与精度

- 错误率——分类错误的样本数占样本总数的比例
- 精度——分类正确的样本数占总数的比例
- 查准率（precision）：
  
  $\frac{TP}{TP+FP}$
- 查全率（recall）:
  $\frac{TP}{TP+FN}$

两者是一对儿矛盾度量

> 举例:
> >商品推荐系统：更关注查准率

> >罪犯检索系统：更关注查全率

 一般情况下根据学习器的预测结果对样例排序，可绘制出P-R图
如下所示：

<img src="https://github.com/Optimus-Prime-X/Markdown-Photos/blob/master/P-R%E5%9B%BE.png?raw=true" width="500" height="500" align=center />

通过对平衡点的比较可以比较模型的优劣，但是不全面，更多使用$F_1$值进行比较：

$F_1 = \frac{2\cdot P \cdot R}{P+R}$ P,R为precision和recall

$F_1$还可以转变为诶有偏好的$F_\beta$

$$BEP = Precision = Recall$$当准确率等于召回率时，定义BEP

### 2.3.3 ROC与AUC

ROC曲线的坐标：

- 横坐标：假正率 $\frac{FP}{FP+TN}$
- 纵坐标：真正率 $\frac{TP}{TP+FN}$

![ROC & AUC](https://github.com/Optimus-Prime-X/Markdown-Photos/blob/master/ROC_AUC.png?raw=true)

### 2.3.4 代价敏感错误率与代价曲线

为了解决*非均等代价*

对不同的分类可以引入代价：

|真实类别|预测|类别|
|---|:----:|:---:|
||第0类|第1类|
|第0类|0|cost2|
|第1类|cost1|0|

代价曲线：

![代价曲线](https://github.com/Optimus-Prime-X/Markdown-Photos/blob/master/%E4%BB%A3%E4%BB%B7%E6%9B%B2%E7%BA%BF.png?raw=true)

TPN: 真正例率 $TPR = \frac{TP}{TP+FN}$ 等价于召回率
FPR: 假正例率 $FPR = \frac{FP}{TN+FP}$ 

> 记忆方法：TP乘积为正，分母为真实的正例总体，分子为TP；FP，乘积为负，分母为全体的反例，分子为FP

真正例率为正确预测为正例的数量与全体正例的比例，假正例率为错误预测为正例的数量占全体反例的比例。一个好的分类器，应该是TPR高，FPR接近0

代价曲线下方的面积为所有条件下学习器的期望总体代价

## 2.4 比较检验

需要考虑的是，学习器A比B在测试集上表现更优，是否是统计意义上的更优，该把握的大小需要通过假设检验

### 2.4.1 假设检验

首先需要考虑，测试集的观测误差率$\hat{e}$与实际的泛化误差率$e$是否为正确估计，可以证明前者是后者的无偏估计。当比较两个分类器的误差率差别时，可以考虑对两个分类器的一系列观测误差率进行做差，得到一系列的$\Delta e$，对这些$\Delta e$进行t-检验。

### 2.4.2 t-检验
  
$$\mu = \frac{1}{k} \sum_{k=1}^{k} (\hat{e_i})$$
$$\sigma^2 = \frac{1}{k-1}\sum_{k=1}^{k}(\hat{e_i}-\mu)^2$$ 

这k个测试错误率看成是对泛化错误率的独立采样，则变量：
$$\tau_t=\frac{\sqrt{k}(\mu-e_0)}{\sigma}$$
服从自由度为k-1的t分布。因此可以对$\Delta e$进行t检验，判断是否拒绝两个分类器泛化误差率相等的假设。

- 交叉验证t-检验

很多情况下，两个学习器进行交叉验证时，样本存在重复采集，使得测试错误率结果并非完全不相关，一般针对这种情况采用5次2折交叉检验，并对单次交叉检验的$\mu$和$\sigma$进行相应的选择。选择如下：
> 单次两折交叉中，使用两个分类器的测试误差平均值$$\mu = \frac{\Delta_1^1+\Delta_1^2}{2}$$
而方差公式$$\sigma_i^2 = (\Delta_i^1-\frac{\Delta_i^1+\Delta_i^2}{2})^2+(\Delta_i^2-\frac{\Delta_i^1+\Delta_i^2}{2})^2$$

### 2.4.3 McNemar检验

二分类问题，两个分类器还可以形成一个“列联表”，表示学习器共同预测为正，共同预测为负等信息，类似混淆矩阵。
||A-正确|A-错误|
|---|---|---|
|B-正确|$e_{00}$|$e_{10}$|
|B-错误|$e_{01}$|$e_{11}$|

如果两个分类器的学习性能相同，那么$|e_{01}-e_{10}|$应该服从正态分布，因此可以进行$\chi$检验：$$\chi = \frac{(|e_{01}-e_{10}|)^2-1}{e_{01}+e_{10}}$$

### 2.4.4 多个数据集上对多个分类器进行比较

- 基于排序的Friedman检验


### 2.4.5 偏差与方差

$$E(f;D)= bias^2(x) + var(x)+\epsilon^2$$

- 偏差：刻画了学习算法的拟合能力
- 方差：度量了训练集的变动引起的学习性能的变化（数据扰动引起的影响）
- 噪声：任何学习算法所能达到的泛化误差下限，刻画了问题本身的难度

一般来说，存在方差和偏差窘境

----
