# 第二章 模型评估与选择
## 2.1 经验误差与过拟合

引起过拟合的可能因素：

- 学习能力过强
  - 决策树扩展分支
  - 神经网络增加训练轮数

## 2.2 模型评估

测试集与训练集互斥
  
有几种方法划分：

- 留出法——尽可能保证分布一致性
- 交叉验证法——十折交叉验证
``` python
sklearn.model_selection.Kflod(n_splits = 3, shuffle = False, random_state = None)
```
n_splits——k折的值
shuffle——对数据是否随机搅动
random_state——随机种子

训练集训练，测试集测试，验证春去了是一个<font color = #ff0000>测试准确度</font>的测量,而测试准确度是一个<font color = #ff0000>高方差估计方法</font>。

- 自助法（bootstrapping）
  
  有放回抽样，抽样到和原始样本同样等级
  适用于<font color = #aaaa>数据集较小</font>的时候

调参与最终模型——计算与性能折中

## 2.3 性能度量

模型好坏：任务需求，算法，数据！
学习器f的预测结果f(x)与y比较

- 回归任务常用“均方误差”：

### 2.3.1 错误率与精度
- 错误率——分类错误的样本数占样本总数的比例
- 精度——分类正确的样本数占总数的比例
- 查准率（precision）：
  
  $\frac{TP}{TP+FP}$
- 查全率（recall）:
   
  $\frac{TP}{TP+FN}$

两者是一对儿矛盾度量

> 举例:
> >商品推荐系统：更关注查准率
> 
> >罪犯检索系统：更关注查全率

 一般情况下根据学习器的预测结果对样例排序，可绘制出P-R图
如下所示：

![P-R图](https://github.com/Optimus-Prime-X/Markdown-Photos/blob/master/P-R%E5%9B%BE.png?raw=true)

通过对平衡点的比较可以比较模型的优劣，但是不全面，更多使用$F_1$值进行比较：

$F_1 = \frac{2\cdot P \cdot R}{P+R}$ P,R为precision和recall

$F_1$还可以转变为诶有偏好的$F_\beta$

### 2.3.3 ROC与AUC

ROC曲线的坐标：

- 横坐标：假正率 $\frac{FP}{FP+TN}$
- 纵坐标：真正率 $\frac{TP}{TP+FN}$

![ROC & AUC](https://github.com/Optimus-Prime-X/Markdown-Photos/blob/master/ROC_AUC.png?raw=true)

### 2.3.4 代价敏感错误率与代价曲线

为了解决*非均等代价*

对不同的分类可以引入代价：

|真实类别|预测|类别|
|---|:----:|:---:|
||第0类|第1类|
|第0类|0|cost2|
|第1类|cost1|0|

代价曲线：

![代价曲线](https://github.com/Optimus-Prime-X/Markdown-Photos/blob/master/%E4%BB%A3%E4%BB%B7%E6%9B%B2%E7%BA%BF.png?raw=true)
